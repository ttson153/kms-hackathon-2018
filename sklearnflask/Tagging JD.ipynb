{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "train = pd.read_csv(\"train.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "test = pd.read_csv(\"test.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = train[\"description\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def job_to_words( raw_job, lang=\"english\"):\n",
    "    # Function to convert a raw job posting to a string of words\n",
    "    # The input is a single string (a raw job description), and \n",
    "    # the output is a single string (a preprocessed job description)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    job_text = BeautifulSoup(raw_job, \"lxml\").get_text() \n",
    "    #\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(u\"[^a-zA-ZàáãạảăắằẳẵặâấầẩẫậèéẹẻẽêềếểễệđìíĩỉịòóõọỏôốồổỗộơớờởỡợùúũụủưứừửữựỳỵỷỹýÀÁÃẠẢĂẮẰẲẴẶÂẤẦẨẪẬÈÉẸẺẼÊỀẾỂỄỆĐÌÍĨỈỊÒÓÕỌỎÔỐỒỔỖỘƠỚỜỞỠỢÙÚŨỤỦƯỨỪỬỮỰỲỴỶỸÝ]\", \" \", job_text) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 3. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(lang))                  \n",
    "    # \n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 5. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set job descriptions...\n",
      "\n",
      "Job 1000 of 4375\n",
      "\n",
      "Job 2000 of 4375\n",
      "\n",
      "Job 3000 of 4375\n",
      "\n",
      "Job 4000 of 4375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the number of jobs based on the dataframe column size\n",
    "num_jobs = train[\"description\"].size\n",
    "# Initialize an empty list to hold the clean jobs\n",
    "clean_train_jobs = []\n",
    "\n",
    "print(\"Cleaning and parsing the training set job descriptions...\\n\")\n",
    "\n",
    "# Loop over each job; create an index i that goes from 0 to the length\n",
    "# of the job list \n",
    "for i in range( 0, num_jobs ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print(\"Job %d of %d\\n\" % ( i+1, num_jobs ))                                                                    \n",
    "\n",
    "    # clean jobs\n",
    "    clean_train_jobs.append( job_to_words( train[\"description\"][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 1000 of 4375\n",
      "\n",
      "Job 2000 of 4375\n",
      "\n",
      "Job 3000 of 4375\n",
      "\n",
      "Job 4000 of 4375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer1 = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 2000,\n",
    "                              ngram_range = (1,4)) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer1.fit_transform(clean_train_jobs)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "# Get the number of jobs based on the dataframe column size\n",
    "num_data = train[\"tags\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean job labels\n",
    "train_labels = []\n",
    "\n",
    "# print \"Cleaning and parsing the training set job labels...\\n\"\n",
    "\n",
    "# Loop over each job; create an index i that goes from 0 to the length\n",
    "# of the job list \n",
    "for i in range( 0, num_data ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print (\"Job %d of %d\\n\" % ( i+1, num_data ))                                                             \n",
    "\n",
    "    # clean out NaN\n",
    "    if type(train[\"tags\"][i]) != str:\n",
    "        train[\"tags\"][i] = ''\n",
    "    train_labels.append(train[\"tags\"][i]) \n",
    "\n",
    "# print \"Sample training labels: \\n\", train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words for job labels...\n",
      "\n",
      "Sample vectorized training labels: \n",
      " [0 0 1 0 0 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating the bag of words for job labels...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Keep words hyphenated\n",
    "pattern = \"(?u)\\\\b[\\\\w-]+\\\\b\"\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer2 = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000,\n",
    "                             token_pattern=pattern) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_labels = vectorizer2.fit_transform(train_labels)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_labels = train_data_labels.toarray()\n",
    "\n",
    "print (\"Sample vectorized training labels: \\n\",train_data_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a few classifiers, then choose one for optimizing next\n",
    "\n",
    "# Import the desired classifiers, splitters, metrics etc.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from time import time\n",
    "\n",
    "# All classifiers are named clf for compatibility with tester.py\n",
    "# Comment out ('#') all classifiers other than the desired one\n",
    "\n",
    "#clf = DecisionTreeClassifier(random_state=42)\n",
    "#clf = KNeighborsClassifier()\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Split data into training and testing sets, using 30% split\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(train_data_features, train_data_labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "clf.fit(features_train,labels_train)\n",
    "labels_train_est = clf.predict(features_train)\n",
    "labels_pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Training: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90       233\n",
      "          1       1.00      0.94      0.97       719\n",
      "          2       1.00      0.87      0.93       430\n",
      "          3       1.00      0.83      0.91       153\n",
      "          4       1.00      0.97      0.99       680\n",
      "          5       1.00      0.92      0.96       610\n",
      "          6       1.00      0.93      0.96       317\n",
      "          7       1.00      0.87      0.93       382\n",
      "          8       1.00      0.77      0.87        48\n",
      "          9       1.00      0.88      0.94       225\n",
      "         10       1.00      0.88      0.94       451\n",
      "         11       1.00      0.89      0.94       529\n",
      "\n",
      "avg / total       1.00      0.91      0.95      4777\n",
      "\n",
      "Results for Testing: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        98\n",
      "          1       0.53      0.15      0.24       324\n",
      "          2       0.48      0.06      0.10       206\n",
      "          3       0.00      0.00      0.00        56\n",
      "          4       0.86      0.63      0.73       290\n",
      "          5       0.80      0.19      0.31       275\n",
      "          6       0.84      0.24      0.37       134\n",
      "          7       0.70      0.05      0.09       142\n",
      "          8       0.00      0.00      0.00        35\n",
      "          9       0.83      0.15      0.25       103\n",
      "         10       0.77      0.05      0.09       218\n",
      "         11       0.46      0.03      0.05       222\n",
      "\n",
      "avg / total       0.62      0.17      0.24      2103\n",
      "\n",
      "total train/test/prediction time: 2.478 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PhucBuiLocal\\PycharmProjects\\ML\\venv\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for Training: \\n\", classification_report(labels_train, labels_train_est))\n",
    "print(\"Results for Testing: \\n\", classification_report(labels_test, labels_pred))\n",
    "print(\"total train/test/prediction time:\", round(time()-t0, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.dat']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf,\"train.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PhucBuiLocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "data = \"\"\"Mô tả\n",
    "- Vận chuyển sản phẩm của công ty đến khách hàng theo sự phân công của quản lý.\n",
    "- Lưu giữ và vận chuyển hàng hóa cẩn thận.\n",
    "- Giao đầy đủ hàng hóa, hóa đơn chứng từ cho khách hàng.\n",
    "- Giải thích thắc mắc của khách hàng liên quan tới sản phẩm.\n",
    "Địa chỉ làm việc: số 7, đại lộ độc lập, KCN Sóng Thần, Dĩ An, Bình Dương\n",
    "Yêu cầu\n",
    "- Nam , có xe gắn máy.\n",
    "- Nhanh nhẹn, sức khỏe tốt\n",
    "- Chăm chỉ, cẩn thận, nghiêm túc với công việc\n",
    "- Ưu tiên ứng viên đã có kinh nghiệm, thành thạo đường tại khu vực\n",
    "- Giao tiếp lịch sự, nhã nhặn\n",
    "Quyền lợi\n",
    "- Môi trường làm việc năng động, nhiều cơ hội và thách thức;\n",
    "- Được tham gia BHXH, BHYT, BHTN;\n",
    "- Được Cty mua bảo hiểm rủi ro 24 / 24;\n",
    "- Các chế độ theo quy định của Luật Lao động và Công ty (sinh nhật, hiếu, hỉ, ốm đau,...)\n",
    "Hồ sơ Ưu tiên nộp hồ sơ trực tuyến qua hệ thống của Timviecnhanh.com\n",
    "Hoặc gửi CV mô tả quá trình học tập và làm việc về email liên hệ.\"\"\"\n",
    "\n",
    "data1 = \"\"\"Mô tả \t\n",
    "\n",
    "- Giao hàng bằng xe máy trong khu vực Q1, Q2, Q3, Q4, Q5, Q7, Bình Thạnh, Phú Nhuận.\n",
    "- Chủ yếu là giao đồ ăn nhẹ nhàng, khách lịch sự hay “tip”.\n",
    "- Ca 15 ngày: 10h30 - 23h30, ngày làm ngày nghỉ xen kẽ.\n",
    "- Ca tối: 18h30 - 23h30.\n",
    "Yêu cầu \t\n",
    "\n",
    "- Trình độ bằng cấp: Không yêu cầu\n",
    "- Rành đường thành phố hoặc có tinh thần học hỏi, chịu khó sẽ training từ từ.\n",
    "- Chăm chỉ, thật thà.\n",
    "- Nhiệt tình, có tinh thần học hỏi, chăm chỉ, đúng giờ.\n",
    "- Độ tuổi tuyển dụng: Dưới 23 tuổi\n",
    "- Giới tính: Nam (Không tuyển nữ)\n",
    "- Ưu tiên có nhà HCM hoặc không bị giới hạn giờ giấc nhà trọ.\n",
    "- Ưu tiên khu vực gần cửa hàng ở phường 22 Bình Thạnh.\n",
    "Quyền lợi \t\n",
    "\n",
    "- Thu nhập tối thiểu từ 4.500.000 trở lên, tiền tip cao, công việc đơn giản.\n",
    "- Hỗ trợ chi phí điện thoại liên lạc, tiền thối và đào tạo đầy đủ các kĩ năng làm việc nhóm, giao tiếp, xử lý vấn đề.\n",
    "- Có các chính sách thưởng hàng tháng.\n",
    "- Du lịch thường niên.\n",
    "- Cơ hội thăng tiến công bằng.\n",
    "- Môi trường trẻ trung năng động, vui vẻ đoàn kết, có làm có chơi có anh em bạn bè đồng nghiệp tận tình hỗ trợ.\n",
    "- Các sếp cực kì lạ và độc.\n",
    "- Đồng nghiệp cực kì đáng yêu và chuyên nghiệp.\n",
    "Hồ sơ \t\n",
    "\n",
    "- CMND photo\n",
    "- Sơ yếu lí lịch\n",
    "- Đơn xin việc\n",
    "- Bản sao hộ khẩu\n",
    "(Tất cả giấy tờ không cần công chứng)\"\"\"\n",
    "\n",
    "data2 = \"\"\"\n",
    "Mô tả \t\n",
    "\n",
    "- Đến ngân hàng giao nhận chứng từ.\n",
    "- Lên hãng tàu lấy B/L, D/ O, hóa đơn, đi giao nhận chứng từ liên quan.\n",
    "- Theo dõi, giám sát quy trình giao nhận hàng hàng hóa.\n",
    "- Xử lý các công việc liên quan.\n",
    "Yêu cầu \t\n",
    "- Nam\n",
    "- Có kinh nghiệm trong lĩnh vực giao nhận Xuất nhập khẩu.\n",
    "- Sẽ được training thêm khi vào làm.\n",
    "- Có phương tiện đi lại.\n",
    "- Tiếp thu nhanh, nhiệt tình trong công việc, hòa đồng, trung thực.\n",
    "Quyền lợi \t\n",
    "\n",
    "- Được hưởng đầy đủ quyền lợi của người lao động theo luật hiện hành.\n",
    "- Được hưởng chế độ thưởng lễ, Tết theo kết quả kinh doanh của công ty.\n",
    "- Được tham gia đào tạo nâng cao chuyên sâu chuyên môn và kỹ năng mềm.\n",
    "- Cơ hội phát triển bản thân và thăng tiến trong tổ chức.\n",
    "- Môi trường làm việc năng động, thân thiện.\n",
    "- Lương: 7 triệu + phụ cấp.\n",
    "Hồ sơ \t\n",
    "\n",
    "- Ưu tiên nộp hồ sơ qua hệ thống timviecnhanh.com\n",
    "- Hoặc gửi trực tiếp về email liên hệ.\n",
    "\"\"\"\n",
    "raw_data = [data, data1, data2]\n",
    "raw_label = [1, 0, 0]\n",
    "raw_train = []\n",
    "for  d in raw_data:\n",
    "    words = job_to_words(data, \"vietnamese\")\n",
    "    raw_train.append(words)\n",
    "train_data_labels = raw_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer1 = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 2000,\n",
    "                              ngram_range = (1,4)) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer1.fit_transform(raw_train)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
